# ðŸ“š Reading: Problems Prompt Engineering Cannot Solve

<p><em>Select the tabs to navigate through the content.</em></p>
<div style="margin: 1em 0%; padding: 10px 15px; border: 2px solid #A2AAAD; background: #ffffff; font-size: 100%; overflow: auto;">
<div class="enhanceable_content tabs">
<ul>
<li><a href="#fragment-1">Introduction</a></li>
<li><a href="#fragment-2">When Not To Prompt</a></li>
<li><a href="#fragment-3">What Could Go Wrong?</a></li>
<li><a href="#fragment-4">Summary</a></li>
<li><a href="#fragment-5">Check Your Understanding</a></li>
</ul>
<div id="fragment-1" style="overflow: auto:;">
<h3>Introduction</h3>
<p>When using chatbots for prompt engineering, it is essential to recognize that while these technologies offer immense potential and advantages, there are certain scenarios where their usage should be approached with caution â€“ or not at all.&nbsp;</p>
<p>This lesson explores various scenarios where prompt engineering with chatbots should not be employed because it is highly likely to lead to an unfavorable outcome. From mental health support to emergencies and legal advice to handling sensitive information and ethical decision-making, these scenarios shed light on the limitations and risks associated with relying solely on automated prompts. Understanding these scenarios helps us recognize the importance of human involvement, expertise, and ethical considerations in areas where prompt engineering may fall short, ultimately guiding us towards responsible and effective use of these technologies.</p>
<h4>Lesson Objectives</h4>
<p>By the end of this lesson, you will be able to&nbsp;</p>
<ul>
<li aria-level="1"><span>Recall potential problems that prompt engineering cannot help solve</span></li>
</ul>
</div>
<div id="fragment-2" style="overflow: auto:;">
<h2>When Not to Prompt</h2>
<p><span>While prompt engineering with chatbots offers a wide range of applications, there are certain scenarios where its usage should be avoided. Here are a few examples:</span></p>
<ol>
<li aria-level="1"><strong>Mental Health Support:</strong><span> Prompt engineering should never be used as a substitute for professional mental health support. Chatbots may lack the empathy and human connection necessary to address complex emotional and psychological needs. It is crucial to prioritize human interaction and ensure individuals receive appropriate and qualified mental health care. Remember our old friend ELIZA? Even though she was considered revolutionary at the time (as a Rogerian Psychologist, nonetheless) there was no chance that she could provide personalized health recommendations. While Chat-GPT is impressive in comparison to ELIZA, this rule still rings true.&nbsp;</span></li>
<li aria-level="1"><strong>Emergency Situations:</strong><span> In critical or life-threatening situations, prompt engineering with chatbots should never be relied upon as the primary means of assistance. Real-time human intervention and immediate emergency services are essential to ensure the safety and well-being of individuals.</span></li>
<li aria-level="1"><strong>Legal or Financial Advice:</strong><span> Prompt engineering should not be used to provide legal or financial advice without the involvement of qualified professionals. Legal and financial matters often require expert analysis, interpretation, and understanding of complex regulations. Relying solely on automated prompts may lead to incorrect or inadequate advice with potential legal or financial consequences.</span></li>
<li aria-level="1"><strong>Highly Sensitive Information:</strong><span> Chatbots powered by prompt engineering should not be utilized for handling highly sensitive personal information without robust security measures in place. Protecting sensitive data is crucial to ensure privacy and prevent unauthorized access or breaches.</span></li>
<li aria-level="1"><strong>Ethical Decision-making: </strong><span>Chatbots should never be solely responsible for making ethical decisions, especially in complex or morally ambiguous situations. Human judgment, critical thinking, and ethical expertise are necessary to navigate intricate ethical dilemmas and ensure responsible decision-making.</span></li>
</ol>
</div>
<div id="fragment-3" style="overflow: auto:;">
<h2>What Could Go Wrong?</h2>
<p>It is crucial to recognize the limitations of prompt engineering with chatbots and prioritize human involvement, expertise, and ethical considerations to provide appropriate support and ensure the well-being and safety of individuals. In the scenarios below, we explore some of the unpredictable problems that can arise when prompt engineering solutions are applied to inappropriate use cases.&nbsp;</p>
<p><span>Letâ€™s review some of our use cases and discuss potential disadvantages.&nbsp;</span></p>
<p>While prompt engineering with chatbots offers a wide range of applications, there are certain scenarios where its usage should be avoided. Here are a few examples:</p>
<ol style="list-style-type: decimal;">
<li><strong>Mental Health Support</strong>: A person struggling with severe depression reaches out to a mental health chatbot for support. The chatbot, solely relying on prompt engineering, fails to recognize the severity of the situation and provides generic responses without offering appropriate intervention or resources. As a result, the individual's condition deteriorates, highlighting the limitations of chatbots in addressing complex mental health needs.</li>
<li><strong>Emergency Situations</strong>: During a medical emergency, a person interacts with a chatbot seeking immediate assistance. However, the chatbot, lacking the ability to assess the urgency and severity of the situation, provides generic advice instead of connecting the person to emergency services. The delay in accessing necessary medical help worsens the outcome and emphasizes the critical need for human intervention in emergency scenarios.</li>
<li><strong>Legal or Financial Advice</strong>: <span>An individual seeks legal advice from a chatbot powered by prompt engineering regarding a complex legal issue. The chatbot generates automated responses based on predefined prompts but fails to grasp the intricacies of the situation, leading to incorrect advice or an incomplete understanding of applicable laws. The reliance on automated prompts without human legal expertise may result in misguided decisions or legal complications.</span></li>
<li><strong>Highly Sensitive Information</strong>: <span>A chatbot integrated into a customer service platform mishandles sensitive personal information shared by a user. Due to inadequate security measures, the chatbot inadvertently exposes confidential data to unauthorized access, leading to privacy breaches and potential harm, such as identity theft or financial fraud.</span></li>
<li><strong>Ethical Decision-Making</strong>: A company implements an AI-driven chatbot to make ethical decisions regarding resource allocation within the organization. However, without human oversight and consideration of broader ethical principles, the chatbot solely relies on predefined prompts and fails to account for nuanced ethical considerations. As a result, the decisions made by the chatbot may overlook important ethical implications and negatively impact stakeholders.</li>
</ol>
<p>These scenarios demonstrate the potential risks and limitations of relying solely on chatbots or prompt engineering in specific contexts, emphasizing the need for human expertise, critical thinking, and ethical considerations to avoid adverse outcomes and ensure responsible decision-making.</p>
</div>
<div id="fragment-4" style="overflow: auto:;">
<h2>Summary</h2>
<p>In this lesson, we discussed several scenarios where the application of prompt engineering with chatbots should be avoided due to potential adverse outcomes.&nbsp;</p>
<p>In the context of mental health support, relying solely on chatbots may lead to inadequate intervention and failure to address the complexities of individual needs. Similarly, in emergencies, chatbots may not possess the capability to assess urgency accurately, potentially delaying necessary assistance.&nbsp;</p>
<p>Legal or financial advice is another area where automated prompts may fall short, lacking the expertise and context required for accurate guidance. Handling highly sensitive information without robust security measures can lead to privacy breaches and unauthorized access.&nbsp;</p>
<p>Moreover, ethical decision-making should not be delegated solely to chatbots, as their reliance on predefined prompts may overlook critical nuances and moral implications. Recognizing these scenarios emphasizes the significance of human involvement, expertise, and ethical considerations in ensuring the responsible and effective use of prompt engineering technologies.</p>
<p>&nbsp;</p>
</div>
<div id="fragment-5" style="overflow: auto:;">
<h3>Check Your Understanding</h3>
<p>In this section, you will be able to quiz yourself on the key takeaways from the readings. These questions will help prepare you for the other assessments in the module.&nbsp;</p>
<p><em>Select the question to view the answer.</em></p>
<details>
<summary style="padding: 15px; font-size: 150%; border: 2px solid #A2AAAD;">True or False? As a prompt engineer, it is your responsibility to consider not only what can go right when we create a solution, but also what can go wrong.</summary>
<p style="margin-left: 10px;">True. It is crucial to recognize the limitations of prompt engineering with chatbots and prioritize human involvement, expertise, and ethical considerations to provide appropriate support and ensure the well-being and safety of individuals</p>
</details><details>
<summary style="padding: 15px; font-size: 150%; border: 2px solid #A2AAAD;">True or False? A potential disadvantage of prompt engineering in personal assistants is the potential loss of privacy and security.</summary>
<p style="margin-left: 10px;">True&nbsp;</p>
</details><details>
<summary style="padding: 15px; font-size: 150%; border: 2px solid #A2AAAD;">List the situations that prompt engineering should never be used for.</summary>
<ol style="list-style-type: decimal;">
<li>Ethical Decision Making</li>
<li>Mental Health</li>
<li>Counseling</li>
<li>Emergency Services</li>
<li>Legal Advice</li>
</ol>
</details></div>
</div>
</div>